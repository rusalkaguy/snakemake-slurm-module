#!/bin/bash
#
# launch snakemake to run jobs on UAB CHEAHA via SLURM
#

# debugging
EXEC="exec"
if [[ "$1" == -*debug ]]; then
    # echo commandline instead of running it
    EXEC="echo CMD: "
    DEBUG=1
    shift 1
fi

#
# SLURM same name
#
# list of SLURM parameters that will come straight from cluster.slurm.cheaha.json/cluster.json with SAME NAME
#
SM_PARAMS="job-name ntasks partition time mail-type error output"

#
# SLURM mapped name
#
# list of SLURM parameters that will come from Snakefile and cluster.slurm.cheaha.json/cluster.json
# with a name mapping
#
SM_ARGS=""

# "threads: 4" attribute directly on the rule, so works correctly for non-cluster case
SM_ARGS="$SM_ARGS --cpus-per-task {threads}"

# "mem-per-cpu-mb" - add units to the name to cut down on errors.  
SM_ARGS="$SM_ARGS --mem-per-cpu {cluster.mem-per-cpu-mb}"

# "mail-user" - set it to our blazerID
SM_ARGS="$SM_ARGS --mail-user $USER@uab.edu"
#
# build SLURM parameter mapping
#
for P in ${SM_PARAMS}; do
    SM_ARGS="$SM_ARGS --$P {cluster.$P}";
done

#
# validate we have a Snakefile
#
if [ ! -e "./Snakefile" ]; then
    echo "ERROR: $0 can not find $PWD/Snakefile"
    exit 1
fi

#
# look for cluster config files
#
# list in order, later files override earlier ones
#

# global config lives with this script
CCONFIGS="--cluster-config $(dirname $0)/cluster.slurm.cheaha.json"

# look for local ones to merge
for LOCAL_CCONFIG in ./cluster.slurm.cheaha.json ./cluster.json ./slurm-config.yaml ./cluster.yaml; do
    if [ -e "$LOCAL_CCONFIG" ]; then 
	CCONFIGS="$CCONFIGS --cluster-config $LOCAL_CCONFIG"; 
	if [ "$DEBUG" == "1" ]; then echo "# found $LOCAL_CCONFIG"; fi;  
    fi
done

#
# our __default__ SLURM error/output paths expect a logs/ subdir in PWD
#
mkdir -p ./logs/
RC=$?;
if [ "$RC" != "0" ]; then exit $RC; fi

#
# actually run snakemake
#
# --latency-wait SECONDS
#   We need this for files created on compute nodes to become reliably visible on the login node where snakemake runs.
#
# --jobs CONCURRENT_JOB_COUNT
#   Number of jobs snakemake can submit to slurm at one time.
#
# --cluster-config FILE.json
#   SLURM parameters to use when submitting jobs for each rule.
#
$EXEC snakemake \
     --latency-wait 45 \
    --jobs 999 \
    $CCONFIGS \
    --cluster "sbatch $SM_ARGS" \
    $*
